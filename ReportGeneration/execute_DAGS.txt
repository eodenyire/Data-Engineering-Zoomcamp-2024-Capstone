Let's create Airflow DAGs to schedule and orchestrate the execution of these Python scripts for generating CSV reports. We'll create separate DAGs for dbt model data extraction and Redshift query data extraction.

Airflow DAG for dbt Model Data Extraction:
python
Copy code
from airflow import DAG
from airflow.operators.python_operator import PythonOperator
from datetime import datetime, timedelta
import your_dbt_model_script

# Define the default arguments for the DAG
default_args = {
    'owner': 'airflow',
    'depends_on_past': False,
    'start_date': datetime(2024, 4, 1),  # Adjust start date as needed
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
}

# Define the DAG
dag = DAG(
    'dbt_model_report_generation',
    default_args=default_args,
    description='A DAG to generate CSV reports from dbt models',
    schedule_interval='@daily',  # Run daily
)

# Define the task to generate CSV report from dbt models
generate_report_task = PythonOperator(
    task_id='generate_dbt_model_report',
    python_callable=your_dbt_model_script.generate_csv_report,
    dag=dag,
)

# Set task dependencies
generate_report_task

Airflow DAG for Redshift Query Data Extraction:
python
Copy code
from airflow import DAG
from airflow.operators.python_operator import PythonOperator
from datetime import datetime, timedelta
import your_redshift_query_script

# Define the default arguments for the DAG
default_args = {
    'owner': 'airflow',
    'depends_on_past': False,
    'start_date': datetime(2024, 4, 1),  # Adjust start date as needed
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
}

# Define the DAG
dag = DAG(
    'redshift_query_report_generation',
    default_args=default_args,
    description='A DAG to generate CSV reports from Redshift queries',
    schedule_interval='@daily',  # Run daily
)

# Define the task to generate CSV report from Redshift queries
generate_report_task = PythonOperator(
    task_id='generate_redshift_query_report',
    python_callable=your_redshift_query_script.generate_csv_report,
    dag=dag,
)

# Set task dependencies
generate_report_task

In these DAGs, we're using the PythonOperator to execute the Python scripts for generating CSV reports. You'll need to replace your_dbt_model_script and your_redshift_query_script with the actual names of your Python script files containing the report generation functions.

Make sure to adjust the start_date and schedule_interval parameters according to your scheduling requirements. These DAGs are set to run daily, but you can customize the schedule as needed.

Once you've defined these DAGs, you can place them in your Airflow DAGs folder, and Airflow will automatically detect and schedule them for execution based on the specified interval. Let me know if you need further assistance or if you have any questions!
